{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch.models.inception_resnet_v1 import get_torch_home\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, extract_face\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn_model = torch.load(\"/home/andy/Desktop/zoom/mtcnn_model\")\n",
    "resnet_model = torch.load(\"/home/andy/Desktop/zoom/resnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mtcnn_model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionPipeline:\n",
    "    \"\"\"Pipeline class for detecting faces in the frames of a video file.\"\"\"\n",
    "    \n",
    "    def __init__(self, detector, n_frames=None, batch_size=60, resize=None):\n",
    "        \"\"\"Constructor for DetectionPipeline class.\n",
    "        \n",
    "        Keyword Arguments:\n",
    "            n_frames {int} -- Total number of frames to load. These will be evenly spaced\n",
    "                throughout the video. If not specified (i.e., None), all frames will be loaded.\n",
    "                (default: {None})\n",
    "            batch_size {int} -- Batch size to use with MTCNN face detector. (default: {32})\n",
    "            resize {float} -- Fraction by which to resize frames from original prior to face\n",
    "                detection. A value less than 1 results in downsampling and a value greater than\n",
    "                1 result in upsampling. (default: {None})\n",
    "        \"\"\"\n",
    "        self.detector = detector\n",
    "        self.n_frames = n_frames\n",
    "        self.batch_size = batch_size\n",
    "        self.resize = resize\n",
    "    \n",
    "    def __call__(self, filename):\n",
    "        \"\"\"Load frames from an MP4 video and detect faces.\n",
    "\n",
    "        Arguments:\n",
    "            filename {str} -- Path to video.\n",
    "        \"\"\"\n",
    "        # Create video reader and find length\n",
    "        v_cap = cv2.VideoCapture(filename)\n",
    "        v_len = int(v_cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "        # Pick 'n_frames' evenly spaced frames to sample\n",
    "        if self.n_frames is None:\n",
    "            sample = np.arange(0, v_len)\n",
    "        else:\n",
    "            sample = np.linspace(0, v_len - 1, self.n_frames).astype(int)\n",
    "\n",
    "        # Loop through frames\n",
    "        faces = []\n",
    "        frames = []\n",
    "        for j in range(v_len):\n",
    "            success = v_cap.grab()\n",
    "            if j in sample:\n",
    "                # Load frame\n",
    "                success, frame = v_cap.retrieve()\n",
    "                if not success:\n",
    "                    continue\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = Image.fromarray(frame)\n",
    "                \n",
    "                # Resize frame to desired size\n",
    "                if self.resize is not None:\n",
    "                    frame = frame.resize([int(d * self.resize) for d in frame.size])\n",
    "                frames.append(frame)\n",
    "\n",
    "                # When batch is full, detect faces and reset frame list\n",
    "                if len(frames) % self.batch_size == 0 or j == sample[-1]:\n",
    "                    faces.extend(self.detector(frames))\n",
    "                    frames = []\n",
    "\n",
    "        v_cap.release()\n",
    "\n",
    "        return faces    \n",
    "\n",
    "\n",
    "def process_faces(faces, resnet):\n",
    "    # Filter out frames without faces\n",
    "    faces = [f for f in faces if f is not None]\n",
    "    faces = torch.cat(faces).to(\"cpu\")\n",
    "\n",
    "    # Generate facial feature vectors using a pretrained model\n",
    "    embeddings = resnet(faces)\n",
    "\n",
    "    # Calculate centroid for video and distance of each face's feature vector from centroid\n",
    "    centroid = embeddings.mean(dim=0)\n",
    "    x = (embeddings - centroid).norm(dim=1).cpu().numpy()\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_pipeline = DetectionPipeline(detector=mtcnn_model, batch_size=60, resize=0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"/home/andy/Downloads/7_tEmd7Zuw_EKqK.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andy/.local/lib/python3.6/site-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
      "/home/andy/.local/lib/python3.6/site-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  boxes = np.array(boxes)\n",
      "/home/andy/.local/lib/python3.6/site-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  probs = np.array(probs)\n",
      "/home/andy/.local/lib/python3.6/site-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  points = np.array(points)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 180.88849425315857\n"
     ]
    }
   ],
   "source": [
    "s =time.time()\n",
    "faces = detection_pipeline(file_name)\n",
    "print(\"Time = {}\".format(time.time()-s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_faces(faces,resnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
